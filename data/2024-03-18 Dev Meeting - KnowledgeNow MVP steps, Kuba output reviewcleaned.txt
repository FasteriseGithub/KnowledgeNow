Speaker 2
It was a little issue with our Internet. There was a news that a cable got destroyed, damaged, like, under sea. So people in West Africa, we had issue with Internet because there was a damage to Internet, cable on that. So there was a little downtime for us. 

Speaker 1
Anyway, so half the weekend was for sort of chilling. The other half was for some focus time. 

Speaker 3
My weekend was, as usual, working whole time. We're preparing for the US client. Yesterday, making the deck till 03:00 a.m. At our time with Pavo. My Sunday ends up like this. 

Speaker 1
This is a big opportunity. I wanted to share some updates, things that are happening this week. Number one, we are having a Gen Xec this week. A big part of what we're going to talk about is profit sharing. Number two, real estate clients. I have finished the first prototype, so I will be sharing it. Also, what comes next is a lot of variations of what they want. Not of this text to calendar automation, but they want leads, right? We have some ideas for lead finding. For real estate, it is lead finding, I guess is one of the most profitable things, most in demand things. Some other updates. The email campaign for real estate is going. We just had two sessions with Nico to make sure that the messaging that we're putting out there is what we want to put out there. 

Speaker 1
We talked about one resource to learn related to Langchain, related to Orag. We also talked about some tasks that we have for completing, for pushing knowledge now forward. Where we're at for the MVP of knowledgenow is from a human prompt and the global context knowledge base. We're going to transfer that information to the context enrichment agent whose job is to fix and enrich a prompt using what it has at its hand. The output is an enriched prompt. The objective optimizing prompt for rag querying. The input is an enriched prompt and the global context. The output are categories or topics of the prompt. Finally the answer agent we will put two in one for this MVP which is formatting the answer depending on the use case and we receive the enriched prompt global context and the evaluator output which is these reranked chunks and it outputs the final answer, formatted the answer that we're going for the MVP is Bartech. 

Speaker 3
I'm just now making commits to my branch with Xray agent and I make a little mess. There is few ideas where to take that further. I have a looks good version. I just baked that right now. 

Speaker 1
Would you like to share screen and share with us? 

Speaker 3
Yeah, it's already on the GitHub, but I can walk you through. Here it is. All right. Can you see my screen? 

Speaker 1
Yes. 

Speaker 3
All right, so I make a little mess over here in the raquery agent folder there is some curious pieces. This would be good to test whether we should add that or not. So let's just see how this one works. If it works fine, then fine, let's iterate later.Speaker 3
So generally it takes the review the context in each prompt, highlight the key concepts, determine the approach for breaking down in the sub queries categorized by team relevance and stuff like that. Create sub queries for each category. Optimize semantic accuracy. It still should be added some those latent space activation somewhere around in here. But let's not overcomplicate for now. Like I wanted to just make draft. 

Speaker 1
Totally. I love how you call it. Looks good. Yeah, it looks good, man. I really like that blue. Whatever vs code is doing there. That blue looks great. 

Speaker 3
That's markdown formatting inside of vs code. That's why I'm using vs code for prompting because I can have more clarity and readability. 

Speaker 1
We won't go there. Yes, lane space activation will always obviously help while we optimize that. While we optimize that, I will get that vector database querying best practices knowledge base kind of for ourselves. Yeah, for the evaluator agents as well. Right? It's like, hey, these are some querying best practices help us evaluate the query and the answer. Given the knowledge base, what could better? Whatever. So we can implement that in different places. 

Speaker 3
Yeah, definitely. I was actually wondering about whether we go with the knowledge graphs, for example, because I've been digging into those materials on the weekend when I had a little time for that. And I mean, I didn't finish the whole course, but I just go through half of it. Oh, cool. 

Speaker 1
I'm about to start it. Yeah. 

Speaker 3
The queries are quite different, I think, than usual. Querying of the pine cone, for example, whether we would have still those natural language queries in here. And in the knowledge graphs, you kind of have this querying, this cipher kind of thing. I don't know if you got into that yet or not. 

Speaker 1
Not that, but tell me more. 

Speaker 3
I mean, they got this cipher querying, sort of more programmatic querying than the natural language querying, I would say. And I was playing with it a little bit. But I just decided we probably go with pine cone first, and then maybe later we go with the knowledge graphs. So. Yeah, I prepared pine cone. 

Speaker 1
Yeah. It's funny how instinct we are again, yesterday, everybody was talking about knowledge graphs, and it seemed that one level up of preparation of the data, of maintenance of the knowledge graph, of the knowledge base, and of querying. So let's get there. Because the interesting thing about the graphs is that the meeting that we had on Thursday and the meeting that we're having right now are deeply related to each other. And those relationships are so much harder to grab only semantically, even when you look at dates and numbers. Right. It's just an LLM interpreting numbers, and we know how that goes. That's not what they do. They don't intuitively know that 27 follows 26. However, with knowledge graphs. Hey, March 27 followed the meeting that we had on March 26. So there's that continuity there. They're deeply related. Query those two documents. 

Speaker 1
If you query one, query the other kind of relationship, and that opens up so much potential. But it need not be what we implement first. In fact, it might not be the best idea to implement it first. When we're looking for performance and standing out in the market with performance, I would very much lean towards that. But hey, it's a developing field. We're still very early, and let's wait for people to do the things wrong so we can follow the best practices for now, as you said, let's continue with knowledge bases and vector databases. 

Speaker 3
Yes. I also stumbled upon something I think could cause the problems we had last time, that there is, I don't know if there is some kind of built in LANC chain prompt to the retrieval, but I found, or maybe you guys just find the same thing I found. There is something like the query prompt is stating, I'm just sending it that in the discord. Like if there is not enough information, say I don't know. 

Speaker 1
Say I don't know. I saw that today. Oh my God. Yeah, I found it on my end too. The multi query notebook, one of the resources I sent, it also says that. And I was like, oh, that's where that came from. We should be able to modify that, right? 

Speaker 3
Yep. I just don't know whether this is a prompt like Kuba put in the code, or it's just like some underlying length changing towards rug. I just dropped that in the discord right now. Okay, so there is also, the first link is for the rag Pinecon multi query something. So there is some documentation on the local host. I bet you got to have this up and running. So I didn't have time to check that out. 

Speaker 1
I missed you on that one. Would you say that one more time? 

Speaker 3
I mean, the first link is, I found some rack pine cone multi query thing in language documentation, so that may be worth to check out. It's having more details. You can see all templates at localhost docs. So I guess you get it up running on your computer and then you can check that out. So I didn't go as deep into that, but the thing I mentioned before for the I don't know thing is the order link and I passed the prompt they used. So I'm actually put that into PFA. So one of the prompts we have in the. I don't know which, but one of them should be from, I think the long one. Actually, this one should be from this. I mean, I put that into PFA, I don't know, prompt and say, expand it to our thing. 

Speaker 1
Yeah. 

Speaker 3
Image of the figma. And there it go with creating. 

Speaker 1
Okay, I'm checking out those resources now. Yes, the pine cone type query, the well, but that just kind of gives you the answers, right? Unique union. 

Speaker 3
Yeah, I'm not sure whether we need that, but I thought maybe there is some programmatic layer to this, not only making AI generate multiple queries and running them one by one, maybe there is a way to have them all sort of launch asynchronously, like simultaneously in parallel. 

Speaker 1
I'm pretty sure that is a thing. Parallel vector database querying. I feel I remember seeing that. Where is the vector database parallel indexing, parallel processing, no, but that's not for the vector side. Yes, there we go. By leveraging parallel processing techniques, distributed vector databases can enhance query performance. Queries can be executed simultaneously on multiple nodes. Techniques such as parallel indexing, parallel query execution, or distributed computing frameworks can facilitate parallelism, parallel query execution and good call. Good call out. Okay, we'll also look into that. Let's get that agent up and running. The transcript which we're going to use this knowledge base are getting finished, so that will be just Kuba. If you can help with getting those process, I'll let you know when they're ready. Let's see. Okay, thank you Vartek. Thank you for sharing. Kuba. 

Speaker 1
Did you get a chance to jump into any resources and what tasks did you get to tackle? Hopefully the Internet issue didn't kill all of your opportunities for jumping into the Internet. 

Speaker 2
Not really. Actually, I was really able to build out the context agents like using the rug, the knowledge bees as a tool, checking out other resources I was not able because the Internet was very bad. Now that he's welcome, I'm going to check the cool. 

Speaker 1
Yeah, they'll definitely help as we get deeper and deeper into rag. So could you show this global context document with the retrieval? Is that implemented yet? 

Speaker 2
Okay. 

Speaker 1
Bartek, could you stop sharing so he gets a chance? 

Speaker 3
All right, I'm also drafting the evaluator agent altro. I think we will get clarity on what exactly evaluator agent should do a little bit later. We have run those context plus extractor agent pairs and have few test cases run through these, then see the output, then we'll iterate on evaluator agent and for now I just will put that into the repo as the first question. 

Speaker 1
Cool. You're just missing an E in the faster ICE names up top. Gone through so many transcripts where faster ICE is just not written. Well, I've seen every possible variation. Let's see. Oh, sorry Kua, I stopped your sharing by mistake. Would you share again? 

Speaker 3
Both agents are now on my branch. 

Speaker 1
Cool. Awesome. 

Speaker 3
The rack query is in the looks good folder and evaluator agent just have one prompt right now. It's like draft looks good because it's looking good. 

Speaker 1
Can we love the looks good? Can we name them differently so we know where to look for each one as long as they have? 

Speaker 3
I will just create a test folder and I will put the ready prompts in the test folder. Ready to test. 

Speaker 1
Do that. 

Speaker 3
I will name it. Ready to test. 

Speaker 1
Ready to test. Cool. We'll have to figure out how we go about the testing. Okay. Yeah, so we're with you. 

Speaker 3
Yeah. 

Speaker 2
So after the splitting two chunks, creating the putting into pine con. So now I created it too. And this tool will be used. Use this tool where you're asking question about meetings, tax activities in an organization called fast Rise. Okay, so this is the prompt and this is the tool. 

Speaker 3
I got to check my hit shared button. 

Speaker 1
Yes. 

Speaker 2
All right, I'm initializing the agents. So this is querying the agent. Let me run all, first of all. So I asked it what the buttex and alu discussing a meeting. So, but discuss the outcome of a developer meeting. The focus of ensuring clarity on tax related to a specific system. They are working on dimension brainstorming idea, potentially using the boats to measure messages and emphasis emphasize the importance of each team member understanding attacks within the system for success. The meeting was centered around planning and organizing the work to be done. 

Speaker 1
Okay, this must have been the first knowledge now meeting, right? Yes. It didn't quite get the name. Yeah, sure. Keep going. 

Speaker 2
So I just added memory so we can query multiple times. So it has memory of previous chart. So I asked what system were they working on. So it's not really. Patek and Aliyah were working on specific system, but the details of the system were not specified in the information provided of the exact system. So I said, can you list out the development tax discussed? So he said, ensuring clarity on which tax of the system they are going to tackle. Focusing on the subcomponent of the component. Setting up each team member for success in the project, restoring idea, creating more space measure success. 

Speaker 1
Okay. 

Speaker 2
So I think I've not really added the global context. Because of the Internet, I was not able to download anything. I thought it was so bad. Okay, I'm going to add it and I'm sure it's going to be much more better. 

Speaker 1
Okay. Yeah. So with the context, this is a good sort of intuitive benchmark of where it's at. It knows of a system. It knows of a system that we're building on. It knows a development call. It knows, it knows. It taught certain phrases like setting up each team member for success, which we definitely emphasized on the subcomponents of the component. So whatever chunks it got, if we could print out the chunks that it gets at each step, or rather for the queries, I think that'd be really useful to figure out where is the fall off and what is the improvement once we make some changes like the better transcript and the global context. So that will be useful for logging. We have to figure out a way to get to more test driven development or mainly test driven development because this is intuitively. 

Speaker 1
Oh, that's a good answer. That's a fine answer. It's just intuitive. Right. We need some data as we iterate. Would you show the outputs to bartech one more time? The ones on top. So this is for our first knowledge noun draft where we spend those 2 hours. 

Speaker 2
So I queried the agent after creating the knowledge and putting it into Pyco rather then based in pycon. So I created an agent. So I queried the agent and I asked what did Bartek and Alino discuss in the meeting? So this was the output. 

Speaker 1
So it interpreted that query asked meeting discussion between Bartek and that was okay. So that was the. 

Speaker 2
Said Bartek and you discussed the outcome of a development meeting. They focused on ensuring clarity on tasks related to a specific system they are working on. They mentioned brainstorming idea, potentially using a bot to measure messages and emphasize the importance of each team member understanding the attacks within the system for success. The meeting was centered around playing and organizing the work to be done. Okay. So I created a memory for it so I can queries multiple. This is, I asked another question, another query. What system were they working on? Like I was asking from the previous conversation, I said Battek and Aliyo were working on a specific system, but the details of the system were not specified in the information provided. So it's still not like a way of some information. 

Speaker 1
So what's that first line? Because that seems like the conversation memory or discuss the outcome of a development meeting related to a specific system. So now the query that is being asked to the vector database is being sort of mixed with the conversation memory. Is that right? Yeah. So go show me the query in that cell right above. Yeah. What system were they working on? But now the knowledge base is being asked the query. Bartek and Alejo discussed the outcome of a development meeting related to a specific system they were working on. So what is it doing with that prompt exactly? Because I don't see though what system were they working on in there? 

Speaker 2
Yeah, that's the chat history. 

Speaker 1
Okay. So it's looking at the chat history and then it's asking what system were they working on? Yeah. Okay, cool. So it thinks that there is no specific name to the actual system. Okay, cool. We can the details of the system. We're not specifying the information provided. So that probably refers to maybe it's chunk size, probably chunk quality, maybe too much noise in the transcript. Okay. And then our actual MVP question, which is can you list out the development tasks discussed? 

Speaker 2
This was the output development tasks discussed in the meeting between BuTEC and you include ensuring clarity on which tasks of the system you are going to tackle, focusing on each of component of the component, setting up each team members of success in the project. Brainstorming idea to knowledge, creating more space using the bots to measure messages. 

Speaker 1
Okay, measure message. 

Speaker 3
This is way too generic. Like, if I wasn't on that meeting, I wouldn't know what the heck is going on. I only know because I was there. 

Speaker 1
Yeah, because he literally helped build it. 

Speaker 3
Yeah. Okay, are we using GPT 3.5 or GPT four? 

Speaker 2
3.5 turbo. 

Speaker 1
Okay, let me change it to. Yeah, let's change it to Ford Turbo and we'll rerun it and see if there's a lift there. 

Speaker 3
Yeah. And are we using the context enrichment in here? 

Speaker 1
No, we're not yet. 

Speaker 2
I told earlier about my Internet, so I'm going to try to fix that today. Like use the context enrichment from brother. 

Speaker 1
Four turbo preview, I think. Let's see what happens. Okay, so you just have an ipy and B, I've been trying to get my vs code to look like that. And of course it can parse ipython notebook. Okay, cool. All these dumb things I'm still learning about. Yeah, I mean, good question. Right. Is it about interpretation of the information or the quality of the information being granted? 

Speaker 3
Yeah. 

Speaker 2
What query do you want to give it? 

Speaker 1
Let's do the same one so we can test against low. 

Speaker 3
Right. I organize the folders, the prompts on the repo. Now there is like the ready to test folder and there is folders for each agent. They're just one prompt. That is the best prompt, as I see with my eyes. Without testing, the contact agent is kind of tested because I put that in the GPTs and I play around with it little. So this is in the prompt live folder as before. So the life is kind of ready more because it's already tested. 

Speaker 1
Please show us in a minute, because you're saying all these names that I'm not familiar with. 

Speaker 3
Maybe I'll just screenshot that for later reference. 

Speaker 1
Length of the chunk is 166. Okay. We'd probably want to increase that, especially with the bigger model I've been reading about. Also, whatever chunk you take, whatever chunk is relevant, get the two chunks before and after. That could make sure that we don't miss valuable information. So that might be something to implement too. Tactics for rag get chunks. 

Speaker 2
Issue with. 

Speaker 1
The API key again, yeah, I'm checking and I haven't gotten any notice. That one should be working. Weird though. Strange. Do you maybe rerun the whole no go one more time? 

Speaker 3
Can you see if it's understandable? Now, in the discord, I make a screenshot with little exploration. 

Speaker 1
Let me see. Thanks. Okay, so here's a new folder structure. Live means already tested in GPT's environment. Cool. Ready to test. I picked best version not tested yet. Okay, we have the context agent, the evaluator agent. Okay, I see what you're saying. So what if it was the other way around? What if there was the evaluator agent, the rack query agent, the context agent, and each of those was a folder that had prompts, live prompts, work in progress and ready to test. So then based on the agent, we can create evaluations and test new ones, replace old ones, stuff like that. Does that make sense? 

Speaker 3
Wait, let me look into that. 

Speaker 1
If this works for your logic. Now that is not the critical part, but just making sure that, for example, Kuba can always find latest version and stuff like that. 

Speaker 3
If you'll see in the prompt, work in progress. I have also like each agent has its own folder for the working files, so that's kind of better for me to work with it. 

Speaker 1
Agent has more testing. 

Speaker 3
Probably do another folder with a full, ready prompt, tested and stuff. 

Speaker 1
Yeah, sounds good. We can always refactor the structure. It's good. Whatever works for you now. It's good. As long as we can all find the prompt easily. As long as we can easily identify. Yes. This is the one we have to use for that agent. That's good. Yes. 

Speaker 2
Okay, it's working now. 

Speaker 1
Yeah. What do you think was the issue? 

Speaker 2
So the API key. So for the first query. Okay, so the API key, I need to fix it every time. Okay, so you say now, in the meeting between Alu and along with Kuba, the discussion appeared to be a casual catch up section, rather than focusing on specific topic or agenda. Initially, there were some issues with alio connection, which were resolved by him deciding not to use edge. 

Speaker 1
Bro, are you kidding me? 

Speaker 2
Vata mentioned that he had just eaten immortality, confusing dinner with lunch accidentally, how he was doing. The overall tone of discussion was informal, friendly, and primarily centered on checking each other. 

Speaker 1
So we know how to fix that and I'll take responsibility for that, which is cleaning the freaking transcript, because that is ridiculous. That is ridiculous. 

Speaker 3
This is about me eating. 

Speaker 1
Yeah, this is about you confusing dinner and lunch. 

Speaker 3
Oh my God. 

Speaker 1
Right? Yeah, we just got the first chunk. Let's see, we're going to have to create a bit of maybe even thread in the knowledge now discord channel to have tactics for rag because it's probably a good idea to make sure that we're being. 

Speaker 3
Chunks. Summarization, like a semantic summarization. Or maybe use the SPR for each chunk. 

Speaker 1
SPR for each chunk. See, there are a lot of times where we need a step by step. I just don't want to compress too much too early. And our evaluator and sort of merger agents are already going to go with compressing. I've been thinking about SPR. I don't think we should leave it out. I'm worried about compressing too early, maybe making chunks bigger, but I don't know. I'm going to write it down and we'll get around to trying it. Bartech spr chunk size overlap size. Okay, if we could have a rag agent, if we could have a rag GPT to help us figure out optimal situations for optimal setup for the type of data that we're working with and the type of project that we're working with, that would be really interesting, like hyperparameter tuning. 

Speaker 3
Guys, this question was actually answered properly. The question was, sorry, I didn't notice a system. There was system all right, because I just read that. What were Barte, Caleb and Cuba discussing? Yeah, we basically discussed that. 

Speaker 1
Yeah, right. We updated that in the beginning. Yeah. I wonder if it's a lost in the middle situation. But okay, no specific development task. Come on, man. Okay, so Kuba, number one for debugging. If we could get chunks retrieved, that would be great. So we can actually figure out where the root cause of this is, regardless of if we have global context. Right. If it's not retrieving the right thing, then we're in trouble. So just to make it clear, none of these criticisms are on you. We're developing an imperfect system together. We're exploring an imperfect system together, built on imperfect building blocks. Langchain still very much on development rag, still very much a field of research. So this is awesome progress. 

Speaker 1
Let's get that global context document in there and let's get some debugging prints so we can all go along, provide suggestions, and keep iterating day by day. I will also write this down and send it your way while we are setting up this system to do it for me. 

Speaker 2
All right, cool. 

Speaker 1
Okay, thank you for sharing. Get, find out which resource you want to dig into. Maybe it's multi query, maybe it's something else. There are a few out there that are really interesting that I shared. And maybe it's in the handbook, in the Lang chain handbook. Maybe even find a chapter that resonates with what you're dealing with now because, you know, a lot of the tools, I'm seeing new tools that you're implementing, and that's really cool how you're going about creating tools. So I'm sure there's a chapter in there that will help you kind of level up day by day. 

Speaker 1
Okay, one more thing I'm going to share, and we'll call it my exploration for all this rack stuff began today, and I grabbed this multi query notebook and I started going line by line, cloning it line by line, command CV as little as possible. And just like literally writing it out, which I noticed, even if I'm changing tabs and just writing the same words, I'm seeing much better way of imprinting it into my mind. So I'm only like a quarter of the way through. But I set up a vector database programmatically, which I hadn't done before, and uploaded the document. So that was fun. 

Speaker 3
But that's not, you know, what's the joke of the best way to remember code? Pen and paper. 

Speaker 1
That is totally true. I guess that's why there were so many pen and paper exams during my computer science degree. We were all pissed at that because it's like, are you kidding me? Are you actually kidding? We're writing computer code in piece of paper with pen. 

Speaker 3
Yeah, but it's actually good. 

Speaker 1
You're wrong, because I don't remember any of it. 

Speaker 3
When I was starting out to learn flutter, I had trouble to remember the code and I just was writing it in the paper until I remembered it. Like a muscle memory, it's kind of different use of hand, but still your brain remembers. 

Speaker 1
It's literally that. It's literally muscle memory. Dead on. I want to make sure that we can all see what this will look like from here to Friday. So, no, there's some stuff that we have to get done, but we are making progress, you guys. So MVP. Ready? Going backwards. We're going to need to figure out a way to evaluate. So figure out evaluation metrics. The evaluator age we talked about. Hey, these were the queries that were given. These were the results. Re rank them. But there's something missing there. It's the what could have been better? What could have been better? I'm not just writing as a question. We'll figure out what it looks like in code. Essentially, what queries could have been better. And maybe that's the V two. Maybe that's the QA agent that is in charge of that. 

Speaker 1
But yeah, make knowledge base with cleaned transcripts. Then going backwards for the rag extractor agent, we're putting in some subtasks we might want a best practices doc to go off of. Not essential for the first test, but definitely we want to create some standard. And for the PFA context enrichment agent, we have the global context document, so it can feed off of that. What tasks are here, that what tasks aren't here? What am I missing here? What do we need to make this system better? For the MVP tactic, E. G. There's get chunks, get previous, and after get previous and following chunk play with chunk size. You said there was research on essentially hyperparameter optimization for this vortex. Did I misunderstand once again? Yeah. Did you mention there was some research on, hey, what would be the ideal chunk size for your application? 

Speaker 3
I think there was something about prompts that the prompts ideally should be like 256, 512, 1024, something like that. 

Speaker 1
The prompts themselves. 

Speaker 3
Yeah, I remember Spencer talking about this in those early workshops and lions accelerator. 

Speaker 1
Right. 

Speaker 3
This is because the chunks of the training data that OpenAI was feeding the hrgpt. 

Speaker 1
So are you talking about the encoding into the embeddings, the vectors being of size, like 1536, et cetera? 

Speaker 3
Yeah, like 512. 

Speaker 1
So we should be good, because, Kuba, I see that you're using the text embedding of GBT three large. Even if you use the small, your application might be faster. And those should be at 1536. But let's get that verified. Vector sizing consistent with, obviously the model that we're using for the GBT with model and across essentially branches, that we're all using the same exercise. 

Speaker 3
And I think we are already multi querying. So now we already have the COVID like multi querying and then assessing the best chunks. We have that in those later agents, right? 

Speaker 1
Right, yes. Rag extractor and evaluator, is that what you meant? 

Speaker 3
Yeah, I think we had some in the big picture. We had those later agents, like not for MVP, that were focusing on choosing the best pieces. One, I think it's after supervisor. 

Speaker 1
Okay, the merger. 

Speaker 3
Yeah, this one. It's crucial that what you said before, to print the chunks so we can actually see how many, for example, is retrieved and what is. And this will be crucial for developing later agents. 

Speaker 1
Yeah, it's a little tricky. We have a transcript, right? So at the start of each person speaking says like a speaker, and even if it's, I read even if it's like speaker one, speaker two, speaker three, and no names, that really helps with performance. So that's part of what I'm working on. So the chunks are just like this is where it gets cut off. So if it's in the middle of a paragraph and that's the chunk that gets retrieved, there's no information or coherence with who is saying it. That feels uneasy with me given the structure of our data. So maybe it's more overlap, maybe it's bigger chunk size with more overlap to make sure that we're getting those speaker names or speaker tags in there. Yeah, there's a million things we can do. So this, let's get this MVP by Friday. 

Speaker 1
Let's get something that we can see the whole pipeline through and we can start taking steps back and identifying the critical points of failure. 

Speaker 3
Yeah. 

Speaker 1
Cool. But yes, good call out on this. 

Speaker 3
I think we should have some test iteration pipeline. Whether Kuba is running the code, seeing the outputs, let's share that somewhere. I don't know. Maybe airtable, maybe screenshots in some channel of discord. We would separate for that. Yeah, very helpful. Iterating on the prompts, seeing actual outputs. 

Speaker 1
I agree. I very much agree. Good call out. I've been getting friendlier with airtable and it feels like at the very least we can start there and we can get more sophisticated, but a place we can even just see collect. I'll try to create a module from airtable sometime this week to just, hey, here's the table name, just put it in there. 

Speaker 3
Also automate that. Add a function to every agent that after outputting the message, send that through API to airtable. And then we could have like this agent said this, this were the inputs and stuff that would be automatically covered. 

Speaker 1
Then let me share what I did here. Maybe it's something like this. 

Speaker 3
1 second. 

Speaker 1
Yep. 

Speaker 2
Sorry guys, my batteries will. I'll catch up with the record. 

Speaker 1
Yes, 1 second. I just wanted to show you, we're going to put the prompts in airtable and the results in something like this where we can say, hey, here's the agent, here's the prompt, here are the results, or here are the chunks printed. We're going to have something like this and we can actually keep track. If we're going to not keep it super programmatic, then at least let's add the systematic aspect and we'll figure out LLM evaluation, which is always still very much the more lagging field of research. Thank you, Kuba. Thank you, Bartech. I'll write down fear to do, and we can move forward. Any questions? What questions can answer? 

Speaker 3
I don't think so. I think we have clearly laid out what are the next steps? We need some more debugging. We need some more control over testing, and that's it, basically, where we'll go from there. 

Speaker 1
Yeah. I think we're in a good place, guys. I think we're moving forward. So let's bang the next agent out. I'll make sure you guys have the resources that you need, including the context, the transcripts, and we got this. The end line of the MVP is insight. So let's keep going. 

Speaker 3
Yeah. 

Speaker 1
Thank you all. 