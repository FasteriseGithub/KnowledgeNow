Speaker 1
Yesterday, Kuba showed me his elank chain code, and I had some few questions for him, but the rest, I think it's pretty clear for me. I'm planning to do that code by myself. Something similar over the weekend. I hope it will be done.

Speaker 2
Great. Also over the weekend, definitely. This will most likely help you with the assignment, which is checking out the resources I tagged you on. I've been looking at different resources for many months, and these are some of the best that I've gathered around Langchain and rag. And there's also a new one, which might be a distraction from this assignment, which is knowledge graphs. And the question has been, if rag is the component of knowledge, now, that is going to be the trickiest to do. Right. How can we improve rag from the foundation? One way that we have come up with is the rag extractor agent. So create grade queries optimized for rag. Another way might be knowledge graphs, which instead of semantic similarity, touches on relationship. So semantic almost relationships. The six resources I shared, specifically the three or four that I tagged you in, find the time to get into and emulate. There's some colab notebooks in there. I want to emphasize how much value and how much growth you're going to get from truly absorbing those resources. 

Speaker 1
Okay. But did you mainly learn from that deep learning course?

Speaker 2
For me, personally, no. For me, Rag has been a very application oriented sort of field I explored as opposed to code oriented. It's been more about finding the applications for which it's useful. That's how knowledge now sort of started making sense in my head. This is what I want. This is the application that I want. How do we get there? And obviously, Langchain engines and rag was the most straightforward way.

Speaker 3
Yeah, I'm also on the journey. I'm also going to be starting to learn because I didn't have time yesterday. Today I scheduled some time to dig into those.

Speaker 2
Number one, we have from here, because Fridays there's not going to be a dev meeting. There is going to be an async update. Kind of like last week, Hubi, how you shared like, hey, this is one thing I'm proud of this week and one thing I'm tackling for next week. We're going to tweak it a little bit. And on Monday we're getting together to teach each other about one resource that we dove into for Langchain. Let's put it tangible, kind of like how Kuba showed. That helps a lot with understanding it. Understanding the failures too. So as we play around with it, what do we have to make better? Becomes a lot more real. And that's the reason to go into test driven development.

Speaker 3
Yeah. And also I was still thinking about this rack query agent, and I think we should get clarity first on the context agent and on the actual context. So I would need the one pager. I mean, I can start working on the rack query agent, but I will not finish it until we have the context agent with the one pager because I feel they are interconnected.

Speaker 2
No, not yet. Let's get that to you by tomorrow. Finished is never going to be heftable. Is a whole different question. I'm hearing you. I'll get you that context one pager by tomorrow. It's very important that we start making this system tangible as days go by and we work on components and subcomponents. This gives us clarity on why and what comes next. What comes before.Speaker 2
And that's what we upload to the knowledge base. Otherwise, we're just going to have bad documents in the knowledge base. Right? 

Speaker 3
So to get some clarity and understanding, you want to actually focus more on that first part right now, let's focus on the cleaning more and let's have some more prompts for cleaning. Maybe outrages. 

Speaker 2
No, actually, what I'm saying is I'm taking care of that part, all right? To make sure that we have good data. And because I've been managing this massive amount of data for months now, I don't want to offload that task into anybody else. It wouldn't make sense. It would just be a waste of time. So what I'm saying is I'm making sure that we have the highest quality data that we can right now, and we might come up with better methods of cleaning and parsing and all that stuff, but making sure that our agents, what we test out of our agents doesn't get noised up, doesn't get dirtied by bad data, which is obviously critical. So allowing you to do your best work with the. 

Speaker 3
You know, I'm thinking also, because I still remember this prompt that was so shocking to us when you make the first one at PFA, about the transcript. Yeah, this one could be actually useful in here. We should dig that out. I'll maybe search for this in our messages. 

Speaker 2
Oh, yeah, it is in our messages, isn't it? Yeah. Let's see. I have the cleaner. I'm going to throw that prompt into PFA so we have actually something decent to work with. Hey, have you thought of this topic? Switch. But have you thought of gifting PFA to this Texas client. 

Speaker 3
Well, I did. Yeah, I did. But we are not sure where is a good time to do that. Right now. He's quite focused on give me the centralized database and I want a chatbot to talk with it. 

Speaker 2
Of course. 

Speaker 3
And the prompt would come after the system is set up and the chatbot is ready, and then he can think about, okay, now I need the prompt, but it's like three months later from now at least. 

Speaker 2
Yeah. 

Speaker 3
Cool. We thought about it, but there is no tangible value, I think, for this client right now, as he's not himself. He is a user of AI, but I don't think he's that much heavy user and that he's rather make me some automation and rather than I use GPT, like this type of AI user, like meta user, let's say. 

Speaker 2
Yeah, I was actually thinking of his investors. 

Speaker 3
I was thinking. 

Speaker 2
I was thinking giving PFA to his investors. How do you double, how could you, for a client, double the price and then being like, oh, yeah, we'll pay double the price. And PFA feels like one of those. Feels like one of those. I can deliver in the tangible, in the product that you need, and I can also deliver massive amounts of value on this other front, it. 

Speaker 3
Yeah, that's kind of a good idea. But we need the PFA swarm, I think because PFA GPT is just too many percentage of a chance that will go wrong on the presentation. It's a funny thing, when I use it for my work, it works like a charm 90% of the time, but when I want to show it to somebody, it's like 20% of the time it works. 

Speaker 2
But that's just live demos, man. That's a function. 

Speaker 3
Yeah, but if we have swarm, we have more control. If we have whole system called PFA, coded up many prompts in the self evaluation stuff, we have way more control. And in GPT, you cannot even set temperature. 

Speaker 2
I hope that you see how knowledge now feeds into the PFA swarm, which feeds into knowledge now, which feeds into PFA swarm. 

Speaker 3
Yeah, that's inception. 

Speaker 2
Yeah, it's beautiful. 

Speaker 3
It's really in the movie from a. 

Speaker 2
Design perspective, it's very beautiful how those two things feed into each other. So we are not meeting tomorrow. I want to then take these next few minutes to tackle some topics. 

Speaker 3
No, I digged up the chat that you shared with me, but there is not a prompt in there, which is just two initial messages and not the actual prompt. 

Speaker 2
Let me find it over here. 

Speaker 3
I bump you the chart. Although I think you have to find it in your chat history of your chart. GPT. Maybe you can at least get the title, I think from this one. Yeah, you have transcript analytics automation. So you can look for this title in your chat histories and you will get the prompt. Because actually this prompt, even though it was the version of PFA, was before latent space engines, it was quite, I mean, like, that was fucking good prompt, dude. 

Speaker 2
Quite good. It was incredible. It literally blew my mind. You saw it live. How much? Let's see. Transcript cleanup prompt. 

Speaker 3
Okay, let me also dig out the video from this call. 

Speaker 2
Yeah, definitely we have the prompt. No, we don't. Part two, autonomous agents. What did you need the full conversation? Because the prompt is right there. 

Speaker 3
No, the final prompt that was conveying all these things, considering, because I only see the two initial messages. 

Speaker 2
Well, where it says part two, that was the prompt where it says integration of sentiment analysis, no inclusion of nonverbal cues analysis. If video or audio elements are available, suggest analyzing nonverbal cues or tone variations for a more nuanced understanding of the meeting atmosphere. This one. 

Speaker 3
I was looking for the black window with the code, and I forget that this version didn't add it. The black window, the code box in the chat you have now. When you're using PFA, you just have the black box. And before, I didn't do that before in this version. 

Speaker 2
Right. 

Speaker 3
It's just text. All right, so this is the problem, actually. Yeah, I just skimmed through that. Yeah. 

Speaker 2
I'm going to share screen one more time. We're going to look at the system as a whole, and we're going to drop off because we know what we have to do. The assignments are clear again. So knowledge, now we know what we're going for. With the MVP discord bot, we can keep track of these assignments, get brainstorming, get clarity on why this assignment matters and where it fits into the whole system. I'm working on the global context. So it's a one pager of each team and briefly, each and Hubi, this is where it's obviously really very relevant to you, which is from a prompt like we've done with Cuba. There's a chat box. You don't need the chat box. You can just do it from the terminal. 

Speaker 2
The PFA context enrichment agent, which is going to be the system prompt of the first agent that you implement. Well, actually kind of the only agent for now what Cuba was doing was there's one agent. A prompt gets asked. The agent asks the vector database queries. The vector database returns the chunks, interprets the chunks and answers the human. So it's essentially a single agent that can query the vector database. That's why Lang chain is so useful, because it does those steps for us. The cool thing is going to be this multi query, and one of the resources that I tagged Bartek and Cuba on is a lesson from the land chain handbook and we might get the author of that book to come speak with us. Actually, this is a wild possibility, but it's a kind of acquaintance of mine. 

Speaker 2
What they came up with is, well, what if you create variations of the query for the vector database? So it asks variations, semantic variations of the same question. So we'll get similar but different chunks in one way enriching the answer from the vector database. So we aim for the rag extractor agent to do exactly that. Bartech, let's try, I'm going to try to get a stubborn knowledge base of all these resources that I sent from which the rag extractor agent might be able to take the most useful concepts, most useful examples to create the prompt where PFA might take this knowledge base of this is the Langchain handbook. This is how you query a knowledge base. This is how you optimize the queries to the knowledge base. 

Speaker 2
Now create a prompt for an AI that does the rag extraction, multi query and all that stuff. So let's test both go crazy with the rag extractor agents and maybe on Monday I send you a knowledge base which then we try to recreate the prompt and maybe we can test to see which techniques does it adopt. And we'll talk about latent space activation. Right. 

Speaker 3
Cool. 

Speaker 2
Yeah, because it's been trained with all this knowledge, like it knows Lang chain. Lang chain is March. It started getting really big even before GPT four came out. So even with the knowledge cut off, how can we activate that lame space and the hyper reasoning abilities? Okay. 

Speaker 3
Just one question. Do we have the length chain already set up in a minor? So I could test those prompts in the agent sequence because currently. 

Speaker 2
You can test, I can get what Cuba built and you can try the, right, so you're asking for the rag extractor agent, right, for this new agent. 

Speaker 3
I mean, look, I'm going to make in this rag extractor agent today and when I'm going to be testing it, I would like to have this sequence where we have already some knowledge base or the transcript uploading somewhere and then I can ask the query and then I can have it go through the context agent first and then through the rug extractor agent because the rug extractor takes as an input what the context agent produces, right? 

Speaker 2
Yes. 

Speaker 3
And we also need the query and also need the transcripts. So that would be essential for testing it properly. 

Speaker 2
Yes, definitely. So we'll get Kuba's current code and then essentially next to that you can add the PFA rag extractor. All right, cool. Part of why the resources for Langchain are going to be useful is, well, how do I chain agents together, which I don't know yet. So that's something I'm going to explore as well, useful for all of us. Does that answer your question? 

Speaker 3
Yeah. 

Speaker 2
Cool. Awesome. And then I'm working on getting some very clean transcripts from this last month. I think the scope of all of this is going to be this last month where we talk about real estate automations, we talk about the personalized outreach system, and we talk about knowledge now. So it's vast and trying. It could get confused with the different projects. Right. So that could be a good test. And then we'll get to the evaluator agent and such. 

Speaker 3
I think we could use the Avi have this idea of Gates data gates. Because he is busy now. Right. I don't see him much. So does that mean we don't have capabilities to do that or we still kind of. This idea goes to the page. 

Speaker 2
I'm going to ask you to go through each of these resources because there's also that in here. It's this one vector search with metadata filtering. 

Speaker 3
All right. I still see the figma, though, but it's in the discord. 

Speaker 2
Right? 

Speaker 3
So I'll just find it because you're sharing Figma still. So I didn't see what you were highlighting yet. 

Speaker 2
Here. So I know it looks a little crazy. I went like super deep on these resources, so please be patient with me and go through these resources. And this one, metadata and vector databases. That is the gated information access. 

Speaker 3
All right. Because I thought it's just some. Avi had some crazy idea for R D or something. 

Speaker 2
Well, yes, and this is the foundation of that. All right, where could Avi take it? Let's bring a demo to him. And I am sure that he could make some time to brainstorm with us where to take it from there because it's a fantastic idea. But again, if we ingest all of these resources, we might get to the point where we can debate with Avi the best methodology rather than just asking him that. I'm sure over the last several months I found many of these resources and then it took like an hour yesterday to curate the right ones for us. 

Speaker 3
Yeah, I actually wanted to start digging into those yesterday, but I was just so already buzed out from the whole day. And I'm also heavily on getting more in depth on the AWS for this project. My brain is just getting double the connections in this week. 

Speaker 2
Well, do yourself a favor and go create a NATO us migration knowledge base. They have many API docs actually, and I just want to quickly show you something that I can't say I came up with it, but it's been super fucking useful. Aws data migration API docs let's see. Migration service documentation. Wonderful. Bam. Xml sitemaps generator create sitemap bam. And it starts looking at every page from the root page. Usually it works. This is weird. API reference usually works. Dandy. Let's see, let's try this one. Live demos. I'm telling you, it never felt like that. It might be Aws has some blockers. Yeah, it feels like it's Aws. Okay, we'll figure it out. But even just like hey, open them all up and put the links in a CSV. What do you do? Go to relevance and it's the simplest thing in the world. 

Speaker 3
Something is cutting you off. 

Speaker 2
Oh sorry. Let's see, maybe not. So it's in API docs scraping. They have kind of a website scraping option and what it does is it'll extract the content if it's too long it will kind of cut some of it. But how you use it is hey, what is the URL and what is the objective of the scraping? But that's for one URL. Well you can run in bulk so you have this link. 

Speaker 3
But your use case is building a knowledge base out of the docs. 

Speaker 2
Exactly. 

Speaker 3
For like AI to help with the project. 

Speaker 2
Something like this, something like that. Something like that. 

Speaker 3
Actually GPT four knows that stuff. I just make a prompt with PFA and basically it helped me to make a whole assessment for the project and what tools we're going to use and feasibility and for sure for actually making the migration. Maybe it would be useful to have that knowledge base. 

Speaker 2
That is my point. 

Speaker 3
But also it's more of for the conversation with the dev. Actually I'm learning more of this way I'm going. And also I'm thinking the GPT four actually knows a lot about AWS so I'm still not sure if that's not overkill to make the knowledge base where he already know most of it in the training data. So I actually make a very good prompt to extract it through latent space and I think it's all right, but I still want to know these things. Not even like wanting for the talking with their developer, but I still need to know this in my head. It's still knowledge that I have to possess to feel good with doing this project. 

Speaker 2
It's been very useful for new APIs. For example, the follow up boss API is 250 pages. And I did that. Exactly that. I created the sitemap enriched by just scraping the website and I fed it to cloud. And cloud has great recall, so it's been super useful in not getting stuck with a new API. And I actually just did it with Firefly's API. Actually, I just did it today and I didn't have to worry about anything because it was a much more complex API than I thought and it has its little tricks. So I'm not an expert with APIs, but now I'm learning so much through scrape the docs or like sidemap scrape the docs, created a GPT, but cloud has been a little nicer. 

Speaker 2
And I usually just go the PFA with the knowledge base, create a GPT, give it the knowledge base again and go from fun. It's been very fun. 

Speaker 3
Yeah, that's a fine workflow for doing this. 

Speaker 2
And the last thing before we log off, have you guys seen Devin? 

Speaker 3
I've seen a video on it. Like David Shapiro says, it's not something. It's like not shocking. His title was David Shapiro video. 

Speaker 2
Yeah, I queued it up. I loved the title. It was so funny digging at Wes Roth because he keeps putting like shocking in his videos. Yeah, it's funny. I wonder what he's testing exactly. I know the YouTube algorithm that goes into that, but he's overdoing it and I think it's on purpose. 

Speaker 3
You mean like a west raw with the. 

Speaker 2
Yeah, yeah. 

Speaker 3
Or was it Dave Shapiro? 

Speaker 2
That's putting shocking in every video. 

Speaker 3
I think it's just now like AI space YouTubers joke. Yeah, it's driving engagement. Because if people see that there is a title like this everywhere, they keep commenting and like, oh, again, like know everybody driving engagement. Actually, the title of the Javi Shapiro from yesterday got me thinking, maybe we should make a title like this for PFA. This is not shocking AI prompt engineer because it will set expectations to the other side. And also later we maybe shock some people. Who knows? 

Speaker 2
Yeah, who knows? I love that. Bring that up with Paige. That's fantastic. 

Speaker 3
Yeah, we're definitely going to do that. 

Speaker 2
Yes. 

Speaker 3
We got to set some calendar actually because we already have around 2 hours of content about PFA. So in the process of making long form and shorts are already there. And like next week or maybe even this weekend, we publish some shorts and definitely need a content calendar for that and start working on titles, strategy for publishing it and going from one step to another. I'm not sure if maybe we have a bigger meeting video. Yeah, definitely. I'm thinking if we should have maybe a bigger meeting for content strategy or I should just talk about this with page alone or we should have think. 

Speaker 2
That that is a fantastic idea. Right now our priority is sales. So all the marketing outreach salespeople are dedicating their time to the email campaign. We're launching it tonight actually, and after that we can make some time. I think it's a fantastic idea. Bartech. Let's all get together and create a content strategy. Yeah, cool. Kubi, if you haven't seen Devin, I'll send you a video. It is shocking. It is a good representation of what we're going for. But Devin is specifically a software engineer we're going for useful for everybody in a company, tailored for anybody in a company. It's a good image of what do we mean by these multi agents working together. The autonomy that these multi agents have. 

Speaker 1
Is this something similar to rack system? 

Speaker 2
It very much is rag and taken to the next level. I do wonder where the Rag component comes in because it's a fantastic software engineer and either it's great latent space activations through the system prompt or actual knowledge bases that are getting preread. So I wonder exactly what it is. I wonder where the multi agents come in because it has to be multi agent. There's no way a system that's that good is built over the LLMs that we currently have. A system that is that capable of meta reasoning, of reasoning over itself, not in a consciousness and self awareness way, but yes, you'll see it in the video. You'll see it in the video. Okay, cool. You guys, I will see you on Monday and I will see you on the chat. 

Speaker 2
We have one assignment for all of us, which is we dig into one resource. And on Monday we essentially demo what we learned from that resource, as basic or as advanced, as specific or as broad as that resource may be. Go over each of the resources, figure out which is the one you want to dig into, and on Monday we talk about it. Each of those is going to help us build better systems. 

Speaker 3
All right. 

Speaker 2
Okay. 

Speaker 1
But I don't think I will be able to attend on the death call on Mondays as usual. 

Speaker 2
Because of the time? 

Speaker 1
No, because of the time. At the time I am in school on the campus. Last time I was on the break. 

Speaker 2
Excuse me, what time do you come back home? 

Speaker 1
What time do I back home? Let me see. It's 05:00 p.m., no. 

Speaker 2
We'Ll do you on Tuesday. Could you send to the scheduling channel what your times look like next week and we'll figure out where everything fits. Okay, I'll tag you in the scheduling channel and you can share like this is where I'm available, this is when I'm not available so we can all work better together. Okay, cool. 

Speaker 1
If the meeting is the same hour as today, I can attend every day of the week but no Monday. 

Speaker 2
Okay, good to know. 

Speaker 1
But every other day is fine for me. 

Speaker 2
The days that we can't attend, we send an update on the development chat. Kuba isn't here so he'll send an written update in the development chat. 

Speaker 3
Okay. 

Speaker 2
And I'll try to make sure that we all have as clear tasks as possible. So even if it's a written update, it makes sense with everything else that. 

Speaker 3
We'Re working with and try to get this code from the Cuba once he makes the update or something. So I would have by the evening something to test on. 

Speaker 2
Sounds good brother. Cuba will be on in about one or 2 hours so we'll make sure that he sends that. All right, cool. I'll see you guys later. 

Speaker 3
Yeah, see Yamani. Bye. 